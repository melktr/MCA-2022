# &#12304; MELANIE KTORA - 2485564 - MCA PROJECT &#12305;
                                                                                                                                         
[__WEEK 1__](https://melktr.github.io/MCA-2022/#week-1) | [__WEEK 2__](https://melktr.github.io/MCA-2022/#week-2) | [__WEEK 3__](https://melktr.github.io/MCA-2022/#week-3) | [__WEEK 4__](https://melktr.github.io/MCA-2022/#week-4) | [__WEEK 5__](https://github.com/melktr/MCA-2022/blob/master/README.md#week-5) |READING WEEK| [__WEEK 7__](https://melktr.github.io/MCA-2022/#week-7) | [__WEEK 8__](https://melktr.github.io/MCA-2022/#week-8) | [__WEEK 9__](https://melktr.github.io/MCA-2022/#week-9) | [__WEEK 10__ ](https://melktr.github.io/MCA-2022/#week-10)

## __WEEK 1__ 
  
### &#10238;THEME

My dataset is based on ***Iannis Xenakis (Greek: Γιάννης "Ιωάννης" Κλέαρχου Ξενάκης)*** , a Romanian-born Greek-French avant-garde composer, music theorist, architect, performance director, and engineer. He revolutionized the application of mathematical models in music, including  stochastic processes, applications of set theory and game theory, and had substantial impact on the growth of electronic and computer music. He fused music and architecture by writing music for specific locations and designing surroundings that could be linked to certain music compositions and performances.

Some of his greatest works include *"Metastaseis" (1953-54)* for orchestra, with independent parts for each musician in the orchestra; percussion works such as *"Psappha" (1975)* and *"Pléades" (1979)*, *"Terretektor " (1966)*, which introduced  multidimensional sound by dispersing musicians among the audience, electronic works created using Xenakis's UPIC system (computerised musical composition tool); and the massive multimedia performances Xenakis dubbed  *"Polytopes"* fusing the ancient greek terms “poly” (“many”) and “topos” (“place”).

![429-IX-photo-André-Villers-coll-Famille-IX-DR -778x1024](https://user-images.githubusercontent.com/113994055/194643258-9c43323b-325f-471b-af29-be8e2b5b59a8.jpg)

### &#10238; Obstacles in interacting with music and music-related data and their display on Iannis Xenakis through Spotify 

###### &#8688; *Access*

Accessing music data has become incredibly simple in recent years, but primarily for well-known, or 'popular,' singers and artists. Iannis Xenakis has built a name for himself as a composer, yet his data is not as extensive as that of a pop singer. Many of his works, such as music, architecture, and photography, are included on the composer's website, as well as his bibliography and biography, old and new performances or exhibits in his honour, archives, and many more. This information, however, is not visible on major sites that distribute his music, such as Spotify, Deezer, and Apple Music.

Albums, popular releases, singles or EPs, compilations, features, featured playlists, songs' names, artist name, album names, duration of songs, composers, genre, year, track number, track time, play count, user ratings, album/single artwork, lyricists, copyright information, and merch are all visible data on Spotify. 

These may appear to be a lot at first glance, but there are some restrictions. The volume of data available on individual categories makes it harder to analyse, and thus contributes to the data being misconstrued. In this situation, the data is reliant on a "computer," which limits the material to what the device, or the device's operator, decides to display.

###### &#8688; *Curation*

Spotify has shifted to machine learning and other algorithmic-based methods. It has developed a service called "Discover Weekly" that customises a playlist of new music for each user every Monday based on their listening patterns. "Discover Weekly" employs three types of recommendation models: **collaborative filtering, natural language processing (NLP), and raw audio.**

The majority of Spotify's curation originates through c**ollaborative filtering**. That means Spotify map out your preferences depending on your listening patterns and then locates other listeners whose maps match the most with yours. Then, Spotify will suggest music which these other users have listened to but you haven't. The second component, **NLP**, analyses the lyrics in songs.Spotify now creates a map of the track rather than a map of the user. Then, similar to collaborative filtering, it will compare the maps of several tracks to find the greatest overlap. The final input is a **raw audio model**. While the first two models are concerned with connecting users with popular songs, raw audio models help assess new music with less data. Spotify analyses fragmented  songs and categorises them based on time signature, key, mode, tempo, and loudness by using convolutional neural networks. 

Spotify lacks curation in the case of Iannis Xenakis. His music is diversified and niche than commercial or mainstream. His intentions lean towards creating musical experiences and compositions performed in specific architectural settings. It is music designed for people to enjoy, which explains why Spotify, his profile isn't as established, and why they curated him only in one of their playlists.

###### &#8688; *Distribution*

In a world when music is publicly available, the most challenging difficulty that music distributors confront today is differentiating themselves by offering the finest music listening experience possible. Finding the appropriate music for each listener is critical to the whole experience. Unfortunately, the burden of quantity falls heavily on the shoulders of the artists. Some streaming services are fantastic at getting independent artists discovered, but the modest sum they pay must be divided between the distributor, record label (if any), and the artist.

This distinguishes Iannis Xenakis. Although his musical work is difficult to distribute as it is mostly experience-based and you can't transfer the emotional aspect, the humanised limited edition performance, and the feeling of the live soundwaves in your ears,  as previously noted, his work is available in a variety of formats. His music is available via streaming services, architectures, structures, exhibits, photography, notation (graphic and notated scores), audio arts, and different organisations continuing on his legacy, including The Philharmonic Orchestra of Paris, Trio Xenakis, the Hellenic Cultural Center, and Ina GRM.

![228-IX-persepolis-photo-Mali -1971-coll-Famille-IX-DR -1024x950](https://user-images.githubusercontent.com/113994055/194643259-4e08845e-591b-42f9-bd97-4fc81dd54c17.jpg) 
  
## __WEEK 2__ 

For week 2, I opted to transcribe the first few bars of Xenakis' "XAƧ" (SAX spelt backwards), which was originally created for the Rascher Saxophone Quartet. Unfortunately, the original score was ineligible for Musescore's PDF converter, thus here is my attempt at generating a clear version of it. 

The images below are from a pdf version of my MuseScore file which you can download [here](https://github.com/melktr/MCA-2022/blob/master/Labs/WEEK%202/XAƧ%20musescore.mscz)

You can also download the PDF version [here](https://drive.google.com/file/d/1Tl6QxiRHQtomofGqelauh-jKVfzgbjgD/view?usp=sharing) 

<img width="1257" alt="Screenshot 2022-10-08 at 2 42 57 AM" src="https://user-images.githubusercontent.com/113994055/194681443-252092a7-8c54-44d6-ae93-84e3b9869bbb.png">
  
## __WEEK 3__ 
  
### &#10238;TASK 1: Generating MusicXML and MEI files

The first part of this week's task was to export our MuseScore files from last week to a MusicXML file, and from it to an MEI file. Both can be accessed through the links below. 

View the MusicXML file [here](https://github.com/melktr/MCA-2022/blob/master/Labs/WEEK%203/XA%C6%A7%20musescore.musicxml) 

View the MEI file [here](https://github.com/melktr/MCA-2022/blob/master/data/XAS.mei)

### &#10238;TASK 2: Rendering the MEI file using Verovio

In order to present my rendered file as a component of this website for the second portion of this week's task, I copied the code from the "verovio.html" file and made the necessary edits. 

View the MEI on VEROVIO  [here](https://melktr.github.io/MCA-2022/verovio.html)
  
## __WEEK 4__ 

For this week's lab, we had to convert our MuseScore file to MIDI, run it using jSymbolic.jar, and then import our CSV file into Excel after making the necessary adjustments. 

This gave me the opportunity to examine the piece's data and produce a graph showing the percentages of the important figures in the pitches' data.

You can download the MIDI file [here](https://github.com/melktr/MCA-2022/blob/master/Labs/WEEK%204/XA%C6%A7%20musescore.mid)

### &#10238; TASK 1: Generating a jSymbolic

I have gathered certain fundamental characteristics, such as the pitches histogram and data-related numbers (such as the number of pitches, pitch classes, range, strong tonal centres, mean pitch, and so on). These characteristics, I believe, can offer helpful insight into this particular piece. This kind of histogram, for instance, provides specific insights on the range and diversity of pitches employed in a piece, which is especially helpful for "XAS" since it contains many short notes but is devoid of numerical data. For instance, if we look at the spectrogram and waveform for "XAS", we can infer the dynamics that shift over time, and velocity. On the other hand, if we know which frequency corresponds to which pitch, we may determine the notes.

![pitches](https://user-images.githubusercontent.com/113994055/195591780-b3126f59-4897-4389-bd33-87e2edea34e3.PNG)

![numbers related to data](https://user-images.githubusercontent.com/113994055/195591803-51425bfa-cd3c-4469-81fa-b1e5aeab1785.PNG)

Here is a link to the [csv file]https://github.com/melktr/MCA-2022/blob/master/Labs/WEEK%204/featurevalues.csv

### &#10238;TASK 2: Generating a piano roll and a pitch histogram

***Piano Roll***

The retrieved piano roll displays the sequence, duration, and pitch of the notes played over time like any other piano roll. It appears to be in line with the other analytical information on this page (as seen when comparing it with the historgram for example, since the commonality of notes presented in the histogram seems to correspond to the piano roll).

![piano roll](https://user-images.githubusercontent.com/113994055/203786138-cf78819c-04c6-43c1-b0d6-420f7c89b1d6.PNG)

***Scatter Plot of Pitches***

Pitch and note length are shown on the extracted scatter plot of pitches. Once more, this looks to be in line with the piece.

![scatter plot](https://user-images.githubusercontent.com/113994055/203786142-474efef2-c795-4340-9db8-ff08ecfa4b98.PNG)

***Histogram of pitch***

The number of times a specific note has been played in a song is indicated by the retrieved pitch histogram. This histogram indicates that F#4 is the most frequent pitch in my composition, which is compatible with the information obtained from jSymbolic because F#4 is comparable to MIDI pitch value 66 (most common pitch).   

![graph](https://user-images.githubusercontent.com/113994055/203786144-ffaf7832-1817-48e9-a049-17a3c8804962.PNG)
  
## __WEEK 5__ 
  
### &#10238;TASK 1: 
These are the metadata components I would use to describe individual MEI files in my GitHub repository since I think they are the most significant:

***Metadata about the file itself:***

* title (plus an optional subtitle if there is one): name of the transcribed piece

* composer: name of the individual(s)/band who created the musical work

* lyricist: name of the individual(s)/band who wrote the lyrics

* arranger: name of the person/band responsible for the particular arrangement of the piece of music

* encoder: name of the individual(s) that encoded the file

* source: source from which the file was transcribed

* size: physical size of the file itself

* publisher: name of the individual/organisation responsible for the publication of the piece of work

* publisher’s location: postal address or at least a city of the publisher

* date: date of publication

* usage restrictions: legal statute of the work regarding its distribution, publication, etc.


***Metadata about the encoding process:***

* editorial changes: description of any editorial practices that happened during the transcription of the file

* project description: describes the purpose for which the file was created


***Metadata about the original piece (if the source of transcription is different from the original piece):***

* title: name of the original piece

* arranger: name of the arranger of the original piece

* composer: name of the individual(s) reponsible for the creation of the original musical work

* lyricist: name of the individual(s) that wrote the lyrics for the original work

* year: year of the orignal work's creation

### &#10238;TASK 2: Modifying the MEI file

The MEI file was updated to contain some of the above listed metadata and can be accessed [here](https://github.com/melktr/MCA-2022/blob/master/Labs/Week%205/XASupdated.mei) .
  
## __WEEK 7__ 

### &#10238;TASK 1: Updating last week's metadata


### &#10238;TASK 2: Rendering the revised metadata

## __WEEK 8__ 

### &#10238;TASK 1: Identifying three music tracks and listing most important metadata about them

![AUDIO FORMAT](https://user-images.githubusercontent.com/113994055/204532980-675df36b-7119-4191-890c-39dacbf2a2eb.PNG)

![METADATA](https://user-images.githubusercontent.com/113994055/204532989-155de08f-abe7-4105-8b17-5dc1c638488d.PNG)

### &#10238;TASK 2: Generating Spectrograms and Waveforms & Describing Advantages of Time-Frequency Analysis Over a Waveform Based One

### &#10238;TASK 2.1: Computing and exporting a spectrogram and waveform for each track from task 1

***SIX CHANSONS FOR PIANO : SPECTOGRAM AND WAVEFORM***
![six chansons for piano spectogram](https://user-images.githubusercontent.com/113994055/204533493-80c59e04-1f04-42f0-b90b-0e615ccca188.png)
![six chansons for piano soundwaves](https://user-images.githubusercontent.com/113994055/204533668-64b4f547-db30-4a11-ac76-e8f7a8782ff2.png)

***PITHOPRAKTA : SPECTOGRAM AND WAVEFORM***
![spectogram 2](https://user-images.githubusercontent.com/113994055/204533888-40e9d640-03f3-4968-bd57-5fd233fe9b31.png)
![pithoprakta soundwaves#](https://user-images.githubusercontent.com/113994055/204533907-524d0fc2-ceaa-43f5-be07-bba78f998149.png)

***XAS : SPECTOGRAM AND WAVEFORM***
![xas spectogram](https://user-images.githubusercontent.com/113994055/204533939-54bea61f-8ad6-43b3-8000-05a39bb035ea.png)
![xas soundwaves](https://user-images.githubusercontent.com/113994055/204533953-e8893a4b-5b42-4e4f-bb12-137c3702e1f8.png)

### &#10238;Task 2.2: Describing advantages of a time-frequency analysis over a waveform-based analysis

The main benefit of a time-frequency analysis over a waveform analysis is that it makes it simpler to glean important details about the particular musical composition. Since each note in a song has a certain frequency, we can roughly identify the notes being played in a song based on a spectrogram, which is a visual depiction of the fluctuation of frequencies over time. On the other hand, a waveform analysis contains a lot of information but does not present it in a comprehensible manner. Although it lacks numerical data, it provides us with the amplitude, or the energy level, at a certain time point, which equates to loudness, and the form of the wave provides us with an indicator of the variations in frequency.

## __WEEK 9__ 
  
### &#10238; TASK 1: Analysing and Extracting Meaning from Audio

I reused the songs from the week 8 assignments for this week's task. The adjustments we had to do in Sonic Visualiser (Spectrogram, Mel Frequency Cepstral Coefficient, and Chromagram) are shown in the screenshots below.

***SIX CHANSONS FOR PIANO : Spectrogram, Mel Frequency Cepstral Coefficient and Chromagram ***
![six chansons for piano spectogram](https://user-images.githubusercontent.com/113994055/204535749-faacec88-47f9-4a5b-901a-25dcc5f98295.png)
![Mel Frequency Cepstral Coefficients](https://user-images.githubusercontent.com/113994055/204535815-a03a21b7-a457-49a4-8d5c-807ce92a0bcf.png)
![Chromagram](https://user-images.githubusercontent.com/113994055/204535827-bb810091-18e0-4f64-a8fb-9c2eef438ca5.png)

***PITHOPRAKTA : Spectrogram, Mel Frequency Cepstral Coefficient and Chromagram ***
![pithopraktaspectogram](https://user-images.githubusercontent.com/113994055/204535926-d9e1be0f-a933-4d3b-bbac-ece511b5e8e9.png)
![Mel Frequency Cepstral Coefficients](https://user-images.githubusercontent.com/113994055/204535940-9da21a49-ebe0-495b-815a-5099be52faae.png)
![Chromagram](https://user-images.githubusercontent.com/113994055/204535949-d1f318f6-33e0-4b7a-ae7a-1995e8535436.png)

***XAS : Spectrogram, Mel Frequency Cepstral Coefficient and Chromagram ***
![xas spectogram](https://user-images.githubusercontent.com/113994055/204536079-7baafd26-232a-4f4e-8b68-6ab2a5185e10.png)
![Mel Frequency Cepstral Coefficients](https://user-images.githubusercontent.com/113994055/204536091-85f1a34d-eadd-4ab1-9a5a-347ae0232a89.png)
![Chromagram](https://user-images.githubusercontent.com/113994055/204536108-8b89b9ac-6ce3-4fe4-854f-8f57f0885144.png)


### &#10238; TASK 2: Computing and visualising histograms of features

###### &#8688; *Task 2.1: Computing and visualising histograms*

###### &#8688; *Histograms computed from spectograms*

###### &#8688; *Histograms computed from MFCCs*

###### &#8688; *Histograms computed from chromagrams*



### &#10238; Task 2.2: Comparing the histograms
I have chosen to compare the histograms generated from the mel frequency cepstral coefficients for each track for this portion of the job. MFCC is typically used to analyse the tone, colour, or quality of timbre. The MFCCs for the three tracks I'm comparing ought to differ as they are all from different genres and don't contain the same instruments. The histograms above show that this is in fact the case. Additionally, track 1 and track 2 histograms appear to be slightly more similar to one another than each of them is to track 3.  This makes sense given that the former are ...genre and ...... tunes with some shared instruments, whilst the later is a .... .However, there are variances across the three sets of histograms, with feature 0 being the most noticeable.
  
## __WEEK 10__ 
  
### &#10238; Audio Similarity and Transcription

### &#10238; TASK 1: Generating a Similarity Matrix

I utilised the same songs from the previous two weeks for this portion of the assignment. The ... genre is represented by tracks 0 through 3, ..... is represented by tracks 4 through 6, and tracks 7-9 are my tunes. The ...music ... is on track 7, ...track .... is on track 8, and...is on track 9. Tracks 0–3 and Track 9 are more similar to one another than any other tracks, which is reflected in the matrix. Despite the fact tha... songs and ... songs aren't all that different from one another, tracks 4 through 8 are also comparable to one another. However, given that  .... and .... are often more upbeat than other genres, this can be explained.
 
###### &#8688; *Chroma features*

![mean chroma features](https://user-images.githubusercontent.com/113994055/204541990-9ff5348b-fe09-4ceb-ae93-776eea531d0a.png)

###### &#8688; *Features as a CSV*
![features](https://user-images.githubusercontent.com/113994055/204542333-892a923a-0c69-46d0-9c02-b7c8cface161.png)

###### &#8688; *Similarity matrix*

 ![heat map](https://user-images.githubusercontent.com/113994055/204542700-b2331765-e2f4-4a34-a4eb-734ecb71a4aa.png)

### &#10238; TASK 2: Transcription

I've chosen to transcribe the "XAS" file I created in week 2 for this work. The MuseScore file was first exported as a wav file, which was then imported into Sonic Visualiser and converted into a piano roll, which was then exported as a MIDI file. MuseScore was used to open the MIDI file and export it as a picture. The transcribed file from this week and the original notated file I created may be compared below.
 
###### &#8688; *SCORE*
<img width="1257" alt="Screenshot 2022-10-08 at 2 42 57 AM" src="https://user-images.githubusercontent.com/113994055/205460357-195719fe-e3af-47f6-846f-38faedff0489.png">

###### &#8688; *PIANO ROLL*
![xassonicvisualizerpiano-1](https://user-images.githubusercontent.com/113994055/203801653-c0c334bb-b9cb-4c43-8508-66a1349f7b87.png)

###### &#8688; *Reflection on accuracy*

The differences between the files are obvious. Despite having four staves in the original file for a saxophone quartet, the tune is entirely created in two staves in the automatically produced version of musescore. Second, the original document lacked a time signature marking it in free time, while the transcription contained a 3/4 time signature. Third, the transcription's tempo is different from that of the source material. The list could go on indefinitely since each individual bar in the two files has a different look. Understandably, the audio quality of the two files differs greatly. Bars .... to around ... in the transcribed file correlate to the section of the file that, in my judgment, sounds the most like the original one.
